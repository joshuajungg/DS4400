{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13715b0-9cab-4eeb-b708-98b48f02558d",
   "metadata": {},
   "source": [
    "**<h3>Problem 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ea3eff-6086-449a-8def-795c98971efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d29823-a07a-4e4b-885d-ffbfb3a5d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COEFFICIENTS:\n",
      "Intercept: 520.4148\n",
      "bedrooms: -12.5220\n",
      "bathrooms: 18.5276\n",
      "sqft_living: 56.7488\n",
      "sqft_lot: 10.8819\n",
      "floors: 8.0437\n",
      "waterfront: 63.7429\n",
      "view: 48.2001\n",
      "condition: 12.9643\n",
      "grade: 92.2315\n",
      "sqft_above: 48.2901\n",
      "sqft_basement: 27.1370\n",
      "yr_built: -67.6431\n",
      "yr_renovated: 17.2714\n",
      "lat: 78.3757\n",
      "long: -1.0352\n",
      "sqft_living15: 45.5777\n",
      "sqft_lot15: -12.9301\n",
      "\n",
      "TRAINING METRICS:\n",
      "MSE: 31486.1678\n",
      "R²: 0.7265\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Drop columns\n",
    "columns_to_drop = ['id', 'date', 'zipcode', 'Unnamed: 0']\n",
    "train_df = train_df.drop(columns=[col for col in columns_to_drop if col in train_df.columns])\n",
    "test_df = test_df.drop(columns=[col for col in columns_to_drop if col in test_df.columns])\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop('price', axis=1)\n",
    "y_train = train_df['price'] / 1000  \n",
    "\n",
    "X_test = test_df.drop('price', axis=1)\n",
    "y_test = test_df['price'] / 1000\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on training set\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# Calculate training metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"COEFFICIENTS:\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "for feature, coef in zip(X_train.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\nTRAINING METRICS:\")\n",
    "print(f\"MSE: {train_mse:.4f}\")\n",
    "print(f\"R²: {train_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab771cd-9303-434f-aca8-6cd7f75dfd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING METRICS:\n",
      "MSE: 57628.1547\n",
      "R²: 0.6544\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTESTING METRICS:\")\n",
    "print(f\"MSE: {test_mse:.4f}\")\n",
    "print(f\"R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c2bcd-36db-4091-b5bd-12b34b7f9f1c",
   "metadata": {},
   "source": [
    "The features that contribute the most to the linear regression model is grade (92.23), lat (78.38), yr_built (-67.64), waterfront (63.74), sqft_living (56.75). These features contributing makes sense since house quality, location, having a waterfront view, and square footage are all obvious drivers of housing price. \n",
    "\n",
    "The training R² of 0.7265 indicates the model explains 72.65% of the variance in house prices, which is moderately good. The testing R² of 0.6544 represents that the model explains 65% of price variation. suggesting some overfitting. The model is decent but there's is definetely room to improve the model.\n",
    "\n",
    "There's a bit of overfitting (R² drops from 0.73 to 0.65). The model generalizes okay overall, though individual predictions can still be off by a fair amount. The model generalizes okay overall, though individual predictions can still be off by a fair amount.\n",
    "\n",
    "The testing MSE (57,628.15) is significantly higher than the training MSE (31,486.17), indicating the model overfits to the training data and doesn't generalize as well to new houses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebcc5f-fe46-49d6-bd6f-73f0bbc01bd2",
   "metadata": {},
   "source": [
    "**<h3>Problem 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe54ba28-1b7c-4afe-87eb-e48b25029419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COEFFICIENTS:\n",
      "Intercept: 520.4148\n",
      "bedrooms: -12.5220\n",
      "bathrooms: 18.5276\n",
      "sqft_living: 56.7488\n",
      "sqft_lot: 10.8819\n",
      "floors: 8.0437\n",
      "waterfront: 63.7429\n",
      "view: 48.2001\n",
      "condition: 12.9643\n",
      "grade: 92.2315\n",
      "sqft_above: 48.2901\n",
      "sqft_basement: 27.1370\n",
      "yr_built: -67.6431\n",
      "yr_renovated: 17.2714\n",
      "lat: 78.3757\n",
      "long: -1.0352\n",
      "sqft_living15: 45.5777\n",
      "sqft_lot15: -12.9301\n",
      "\n",
      "TRAINING METRICS:\n",
      "MSE: 31486.1678\n",
      "R²: 0.7265\n",
      "\n",
      "TESTING METRICS:\n",
      "MSE: 57628.1547\n",
      "R²: 0.6544\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "X_train_np = X_train_scaled\n",
    "y_train_np = y_train.to_numpy(dtype=float)\n",
    "X_test_np = X_test_scaled\n",
    "y_test_np = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Add bias term \n",
    "X_train_b = np.c_[np.ones((X_train_np.shape[0], 1)), X_train_np]\n",
    "X_test_b = np.c_[np.ones((X_test_np.shape[0], 1)), X_test_np]\n",
    "\n",
    "# Closed-form solution\n",
    "theta = np.linalg.pinv(X_train_b) @ y_train_np\n",
    "\n",
    "# Function to predict on a new testing point\n",
    "def predict_point(x_new, theta):\n",
    "    x_new = np.array(x_new, dtype=float).reshape(1, -1)\n",
    "    x_new_b = np.c_[np.ones((1, 1)), x_new]\n",
    "    return float(x_new_b @ theta)\n",
    "\n",
    "y_train_pred_cf = X_train_b @ theta\n",
    "y_test_pred_cf = X_test_b @ theta\n",
    "\n",
    "# Calculate metrics\n",
    "mse_train_cf = mean_squared_error(y_train_np, y_train_pred_cf)\n",
    "r2_train_cf = r2_score(y_train_np, y_train_pred_cf)\n",
    "mse_test_cf = mean_squared_error(y_test_np, y_test_pred_cf)\n",
    "r2_test_cf = r2_score(y_test_np, y_test_pred_cf)\n",
    "\n",
    "print(\"\\nCOEFFICIENTS:\")\n",
    "print(f\"Intercept: {theta[0]:.4f}\")\n",
    "for feature, coef in zip(X_train.columns, theta[1:]):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\nTRAINING METRICS:\")\n",
    "print(f\"MSE: {mse_train_cf:.4f}\")\n",
    "print(f\"R²: {r2_train_cf:.4f}\")\n",
    "\n",
    "print(f\"\\nTESTING METRICS:\")\n",
    "print(f\"MSE: {mse_test_cf:.4f}\")\n",
    "print(f\"R²: {r2_test_cf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e4afd7-ad2b-4207-840b-8739934430cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING SET:\n",
      "  Closed-Form MSE:    31486.1678\n",
      "  Sklearn MSE:        31486.1678\n",
      "  Closed-Form R²:         0.7265\n",
      "  Sklearn R²:             0.7265\n",
      "\n",
      "TESTING SET:\n",
      "  Closed-Form MSE:    57628.1547\n",
      "  Sklearn MSE:        57628.1547\n",
      "  Closed-Form R²:         0.6544\n",
      "  Sklearn R²:             0.6544\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAINING SET:\")\n",
    "print(f\"  Closed-Form MSE:  {mse_train_cf:>12.4f}\")\n",
    "print(f\"  Sklearn MSE:      {train_mse:>12.4f}\")\n",
    "print(f\"  Closed-Form R²:   {r2_train_cf:>12.4f}\")\n",
    "print(f\"  Sklearn R²:       {train_r2:>12.4f}\")\n",
    "\n",
    "print(\"\\nTESTING SET:\")\n",
    "print(f\"  Closed-Form MSE:  {mse_test_cf:>12.4f}\")\n",
    "print(f\"  Sklearn MSE:      {test_mse:>12.4f}\")\n",
    "print(f\"  Closed-Form R²:   {r2_test_cf:>12.4f}\")\n",
    "print(f\"  Sklearn R²:       {test_r2:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1ad37-8225-4f34-bb9a-13d09f7d003b",
   "metadata": {},
   "source": [
    "The closed-form implementation produces identical results to sklearn's Linear Regression on both training and testing sets. The training MSE(31,486.17) and R² (0.7265), as well as testing MSE (57,628.15) and R²(0.6544), match exactly across both methods. This confirms that my implementation correctly applies the least squares formula which is the same mathematical solution used by sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af304328-1dcb-4820-b2b0-b1013697b716",
   "metadata": {},
   "source": [
    "**<h3>Problem 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9ff1c6-bf03-45e4-975c-57b85498b14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression Results (Feature: sqft_living)\n",
      "\n",
      "Polynomial Degree p = 1:\n",
      "  Training MSE:    57947.5262\n",
      "  Training R²:         0.4967\n",
      "  Testing MSE:     88575.9785\n",
      "  Testing R²:          0.4687\n",
      "\n",
      "Polynomial Degree p = 2:\n",
      "  Training MSE:    54822.6651\n",
      "  Training R²:         0.5238\n",
      "  Testing MSE:     71791.6795\n",
      "  Testing R²:          0.5694\n",
      "\n",
      "Polynomial Degree p = 3:\n",
      "  Training MSE:    53785.1947\n",
      "  Training R²:         0.5329\n",
      "  Testing MSE:     99833.4838\n",
      "  Testing R²:          0.4012\n",
      "\n",
      "Polynomial Degree p = 4:\n",
      "  Training MSE:    52795.7748\n",
      "  Training R²:         0.5415\n",
      "  Testing MSE:    250979.2743\n",
      "  Testing R²:         -0.5053\n",
      "\n",
      "Polynomial Degree p = 5:\n",
      "  Training MSE:    52626.1120\n",
      "  Training R²:         0.5429\n",
      "  Testing MSE:    570616.9148\n",
      "  Testing R²:         -2.4225\n"
     ]
    }
   ],
   "source": [
    "feature_name = \"sqft_living\"\n",
    "feature_index = list(X_train.columns).index(feature_name)\n",
    "\n",
    "X_train_single = X_train_scaled[:, [feature_index]]\n",
    "X_test_single = X_test_scaled[:, [feature_index]]\n",
    "\n",
    "y_train_poly = y_train.to_numpy(dtype=float)\n",
    "y_test_poly = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Create polynomial features\n",
    "def build_polynomial_matrix(X, degree):\n",
    "    poly_terms = [X ** power for power in range(1, degree + 1)]\n",
    "    return np.concatenate(poly_terms, axis=1)\n",
    "\n",
    "# Fit polynomial regression using closed-form solution\n",
    "def fit_polynomial_model(X_data, y_data, degree):\n",
    "    X_poly = build_polynomial_matrix(X_data, degree)\n",
    "    X_with_bias = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]\n",
    "    coefficients = np.linalg.pinv(X_with_bias) @ y_data\n",
    "    return coefficients\n",
    "\n",
    "# Make predictions using learned coefficients\n",
    "def make_predictions(X_data, coefficients, degree):\n",
    "    X_poly = build_polynomial_matrix(X_data, degree)\n",
    "    X_with_bias = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]\n",
    "    predictions = X_with_bias @ coefficients    \n",
    "    return predictions\n",
    "\n",
    "# Train model and compute MSE and R² metrics for given polynomial degree\n",
    "def compute_metrics(X_train_data, y_train_data, X_test_data, y_test_data, degree):\n",
    "    coefficients = fit_polynomial_model(X_train_data, y_train_data, degree)\n",
    "    train_predictions = make_predictions(X_train_data, coefficients, degree)\n",
    "    test_predictions = make_predictions(X_test_data, coefficients, degree)\n",
    "    \n",
    "    train_mse_value = mean_squared_error(y_train_data, train_predictions)\n",
    "    train_r2_value = r2_score(y_train_data, train_predictions)\n",
    "    \n",
    "    test_mse_value = mean_squared_error(y_test_data, test_predictions)\n",
    "    test_r2_value = r2_score(y_test_data, test_predictions)\n",
    "    \n",
    "    return {\n",
    "        'p': degree,\n",
    "        'MSE_train': train_mse_value,\n",
    "        'R2_train': train_r2_value,\n",
    "        'MSE_test': test_mse_value,\n",
    "        'R2_test': test_r2_value\n",
    "    }\n",
    "\n",
    "polynomial_degrees = [1, 2, 3, 4, 5]\n",
    "model_results = []\n",
    "\n",
    "print(f\"Polynomial Regression Results (Feature: {feature_name})\")\n",
    "\n",
    "for deg in polynomial_degrees:\n",
    "    metrics = compute_metrics(X_train_single, y_train_poly, \n",
    "                              X_test_single, y_test_poly, deg)\n",
    "    model_results.append(metrics)\n",
    "    \n",
    "    print(f\"\\nPolynomial Degree p = {deg}:\")\n",
    "    print(f\"  Training MSE:  {metrics['MSE_train']:>12.4f}\")\n",
    "    print(f\"  Training R²:   {metrics['R2_train']:>12.4f}\")\n",
    "    print(f\"  Testing MSE:   {metrics['MSE_test']:>12.4f}\")\n",
    "    print(f\"  Testing R²:    {metrics['R2_test']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e80ed3-4424-4b9b-88e4-2d60a115fc18",
   "metadata": {},
   "source": [
    "As the polynomial degree increases from 1 to 5, the training MSE decreases from 57,947.53 to 52,626.11 and training R² increases from 0.497 to 0.543, indicating that higher-degree polynomials fit the training data more closely. On the testing set, however, the pattern is different. Performance improves from degree 1 to degree 2, the testing MSE decreases from 88,575.98 to 71,791.68, and R² increases from 0.469 to 0.569, showing that a quadratic relationship captures the data better than a linear one. For degrees 3 and higher, testing performance decreases sharply. At degree 4, testing MSE jumps to 250,979.27 with R² = -0.505, and at degree 5, testing MSE reaches 570,616.91 with R² = -2.42. Negative R² values indicate the model performs worse than simply predicting the mean price, demonstrating severe overfitting. The optimal polynomial degree appears to be p = 2, which achieves the best balance between model complexity and generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b395165-98a3-4186-83b8-14852c7f72d0",
   "metadata": {},
   "source": [
    "**<h3>Problem 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e287f74-5ec3-4c46-ac53-8833f304e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent to optimize theta\n",
    "def train_with_gradient_descent(X_matrix, y_vector, learning_rate, iterations):\n",
    "    num_samples, num_features = X_matrix.shape\n",
    "    \n",
    "    parameters = np.zeros(num_features)\n",
    "    \n",
    "    # Gradient descent iterations\n",
    "    for iteration in range(iterations):\n",
    "        predictions = X_matrix @ parameters\n",
    "        residuals = predictions - y_vector\n",
    "        grad = (X_matrix.T @ residuals) / num_samples\n",
    "        parameters = parameters - learning_rate * grad\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Calculate performance metrics for given parameters\n",
    "def calculate_performance_metrics(X_train_matrix, y_train_vector, X_test_matrix, y_test_vector, parameters):\n",
    "    train_preds = X_train_matrix @ parameters\n",
    "    test_preds = X_test_matrix @ parameters\n",
    "    \n",
    "    performance = {\n",
    "        \"train_mse\": mean_squared_error(y_train_vector, train_preds),\n",
    "        \"train_r2\": r2_score(y_train_vector, train_preds),\n",
    "        \"test_mse\": mean_squared_error(y_test_vector, test_preds),\n",
    "        \"test_r2\": r2_score(y_test_vector, test_preds)\n",
    "    }\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a055759-2b37-4baa-bfb9-7d00ad5e96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate α = 0.01, Iterations = 10\n",
      "  θ₀ (intercept): 49.7610\n",
      "  First 5 coefficients: [ 7.8825 12.8892 19.4712  3.6566  6.196 ]\n",
      "  Train MSE: 294798.7336, Train R²: -1.5604\n",
      "  Test MSE: 350525.0973, Test R²: -1.1024\n",
      "\n",
      "Learning Rate α = 0.01, Iterations = 50\n",
      "  θ₀ (intercept): 205.5607\n",
      "  First 5 coefficients: [12.5155 25.923  47.9139  5.4566 11.6682]\n",
      "  Train MSE: 138295.9152, Train R²: -0.2011\n",
      "  Test MSE: 170376.6687, Test R²: -0.0219\n",
      "\n",
      "Learning Rate α = 0.01, Iterations = 100\n",
      "  θ₀ (intercept): 329.9262\n",
      "  First 5 coefficients: [ 6.0354 23.8274 54.5913  3.6622 11.1315]\n",
      "  Train MSE: 70118.9866, Train R²: 0.3910\n",
      "  Test MSE: 97486.2448, Test R²: 0.4153\n",
      "\n",
      "Learning Rate α = 0.1, Iterations = 10\n",
      "  θ₀ (intercept): 338.9574\n",
      "  First 5 coefficients: [ 5.7014 23.6994 55.1981  3.4062 11.0086]\n",
      "  Train MSE: 66499.3155, Train R²: 0.4224\n",
      "  Test MSE: 93559.2950, Test R²: 0.4388\n",
      "\n",
      "Learning Rate α = 0.1, Iterations = 50\n",
      "  θ₀ (intercept): 517.7327\n",
      "  First 5 coefficients: [-11.2983  16.5754  57.6457   4.5065   8.4433]\n",
      "  Train MSE: 31578.9781, Train R²: 0.7257\n",
      "  Test MSE: 58012.3167, Test R²: 0.6521\n",
      "\n",
      "Learning Rate α = 0.1, Iterations = 100\n",
      "  θ₀ (intercept): 520.4010\n",
      "  First 5 coefficients: [-12.4453  17.6066  57.1937   7.8178   8.0081]\n",
      "  Train MSE: 31497.6923, Train R²: 0.7264\n",
      "  Test MSE: 57725.1857, Test R²: 0.6538\n",
      "\n",
      "Learning Rate α = 0.5, Iterations = 10\n",
      "  θ₀ (intercept): 519.9066\n",
      "  First 5 coefficients: [-2639.8475 -3888.6531 -4273.1085 -1655.3395 -2416.5502]\n",
      "  Train MSE: 611829862.7599, Train R²: -5312.9211\n",
      "  Test MSE: 685023079.3421, Test R²: -4107.6524\n",
      "\n",
      "Learning Rate α = 0.5, Iterations = 50\n",
      "  θ₀ (intercept): 520.4174\n",
      "  First 5 coefficients: [-4.3154e+11 -6.4121e+11 -7.1112e+11 -2.7255e+11 -3.9814e+11]\n",
      "  Train MSE: 16494955142449983708463104.0000, Train R²: -143263505624900960256.0000\n",
      "  Test MSE: 18420828390359858157715456.0000, Test R²: -110485009051377369088.0000\n",
      "\n",
      "Learning Rate α = 0.5, Iterations = 100\n",
      "  θ₀ (intercept): 48782298.1976\n",
      "  First 5 coefficients: [-8.0212e+21 -1.1918e+22 -1.3218e+22 -5.0659e+21 -7.4003e+21]\n",
      "  Train MSE: 5698751634744967634835415191328268266437083136.0000, Train R²: -49495323256631576445092355840874144333824.0000\n",
      "  Test MSE: 6364111026359970687384354528826019347351732224.0000, Test R²: -38170860150856429844526095077158657458176.0000\n",
      "\n",
      "SUMMARY TABLE\n",
      " Learning Rate  Iterations    Train MSE      Train R²     Test MSE       Test R²\n",
      "          0.01          10 2.947987e+05 -1.560413e+00 3.505251e+05 -1.102390e+00\n",
      "          0.01          50 1.382959e+05 -2.011404e-01 1.703767e+05 -2.189040e-02\n",
      "          0.01         100 7.011899e+04  3.909961e-01 9.748624e+04  4.152940e-01\n",
      "          0.10          10 6.649932e+04  4.224340e-01 9.355929e+04  4.388472e-01\n",
      "          0.10          50 3.157898e+04  7.257273e-01 5.801232e+04  6.520519e-01\n",
      "          0.10         100 3.149769e+04  7.264333e-01 5.772519e+04  6.537741e-01\n",
      "          0.50          10 6.118299e+08 -5.312921e+03 6.850231e+08 -4.107652e+03\n",
      "          0.50          50 1.649496e+25 -1.432635e+20 1.842083e+25 -1.104850e+20\n",
      "          0.50         100 5.698752e+45 -4.949532e+40 6.364111e+45 -3.817086e+40\n"
     ]
    }
   ],
   "source": [
    "X_train_with_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_with_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "y_train_array = y_train.to_numpy(dtype=float)\n",
    "y_test_array = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Define experiment parameters\n",
    "learning_rate_values = [0.01, 0.1, 0.5]\n",
    "iteration_counts = [10, 50, 100]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for lr in learning_rate_values:\n",
    "    for iter_count in iteration_counts:\n",
    "        # Train model\n",
    "        learned_params = train_with_gradient_descent(\n",
    "            X_train_with_bias,\n",
    "            y_train_array,\n",
    "            learning_rate=lr,\n",
    "            iterations=iter_count\n",
    "        )\n",
    "        \n",
    "        # Evaluate performance\n",
    "        metrics = calculate_performance_metrics(\n",
    "            X_train_with_bias, y_train_array,\n",
    "            X_test_with_bias, y_test_array,\n",
    "            learned_params\n",
    "        )\n",
    "        \n",
    "        # Print theta values\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "        print(f\"\\nLearning Rate α = {lr}, Iterations = {iter_count}\")\n",
    "        print(f\"  θ₀ (intercept): {learned_params[0]:.4f}\")\n",
    "        print(f\"  First 5 coefficients: {learned_params[1:6]}\")\n",
    "        print(f\"  Train MSE: {metrics['train_mse']:.4f}, Train R²: {metrics['train_r2']:.4f}\")\n",
    "        print(f\"  Test MSE: {metrics['test_mse']:.4f}, Test R²: {metrics['test_r2']:.4f}\")\n",
    "        \n",
    "        # Store for table\n",
    "        experiment_results.append({\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Iterations\": iter_count,\n",
    "            \"Train MSE\": metrics[\"train_mse\"],\n",
    "            \"Train R²\": metrics[\"train_r2\"],\n",
    "            \"Test MSE\": metrics[\"test_mse\"],\n",
    "            \"Test R²\": metrics[\"test_r2\"]\n",
    "        })\n",
    "\n",
    "print(\"\\nSUMMARY TABLE\")\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c10ad-0686-4e9b-ac39-800cdb40a26a",
   "metadata": {},
   "source": [
    "For α = 0.01, the algorithm improves as iterations increase. The training MSE decreases from 294,798 to 70,119, and R² increases from -1.56 to 0.39 by 100 iterations, showing slow progress. For α = 0.1, the algorithm converges much faster and achieves the best performance. By 50 iterations, training R² reaches 0.7257, and by 100 iterations it achieves 0.7264, nearly matching the optimal solution of 0.7265. The testing R² also stabilizes at 0.6538, demonstrating effective convergence within 50-100 iterations. For α = 0.5, the algorithm diverges completely. MSE is 10⁴⁵ and R² becomes -10⁴⁰, showing the learning rate is far too large. In conclusion, gradient descent converges when the learning rate is appropriately chosen (α = 0.1), requires around 50-100 iterations to reach the optimal solution, and fails when the learning rate is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a26754-ade1-4675-8fed-04efaf0e86f9",
   "metadata": {},
   "source": [
    "**<h3>Problem 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "531ba9e0-56e2-4c8d-92f7-b7923ec37e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_with_gd(X_matrix, y_vector, learning_rate, iterations, lambda_param):\n",
    "    num_samples, num_features = X_matrix.shape\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = np.zeros(num_features)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Predictions\n",
    "        predictions = X_matrix @ parameters\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = predictions - y_vector\n",
    "        \n",
    "        # Standard gradient\n",
    "        grad = (X_matrix.T @ residuals) / num_samples\n",
    "        \n",
    "        # Add ridge penalty (L2 regularization) - don't penalize intercept\n",
    "        parameters_for_penalty = parameters.copy()\n",
    "        parameters_for_penalty[0] = 0  # Exclude intercept from penalty\n",
    "        ridge_penalty = (2 * lambda_param / num_samples) * parameters_for_penalty\n",
    "        grad = grad + ridge_penalty\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = parameters - learning_rate * grad\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f4bd89-0826-4baa-9978-91ef85e0cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression (λ=0):\n",
      "  Intercept:   1.1441\n",
      "  Slope:       1.9378\n",
      "  MSE:         1.7147\n",
      "  R²:          0.7517\n",
      "\n",
      "Ridge Regression (λ=1):\n",
      "  Intercept:   1.1440\n",
      "  Slope:       1.9343\n",
      "  MSE:         1.7151\n",
      "  R²:          0.7517\n",
      "\n",
      "Ridge Regression (λ=10):\n",
      "  Intercept:   1.1435\n",
      "  Slope:       1.9031\n",
      "  MSE:         1.7200\n",
      "  R²:          0.7510\n",
      "\n",
      "Ridge Regression (λ=100):\n",
      "  Intercept:   1.1385\n",
      "  Slope:       1.6387\n",
      "  MSE:         1.8658\n",
      "  R²:          0.7299\n",
      "\n",
      "Ridge Regression (λ=1000):\n",
      "  Intercept:   1.1131\n",
      "  Slope:       0.6859\n",
      "  MSE:         3.9471\n",
      "  R²:          0.4285\n",
      "\n",
      "Ridge Regression (λ=10000):\n",
      "  Intercept:   1.1022\n",
      "  Slope:       0.1007\n",
      "  MSE:         6.4353\n",
      "  R²:          0.0683\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sample_size = 1000\n",
    "X_values = np.random.uniform(low=-2, high=2, size=sample_size)\n",
    "noise_values = np.random.normal(loc=0, scale=np.sqrt(2), size=sample_size)\n",
    "y_values = 1 + 2 * X_values + noise_values\n",
    "\n",
    "design_matrix = np.column_stack([np.ones(sample_size), X_values])\n",
    "\n",
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    design_matrix, y_values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "max_iterations = 5000\n",
    "regularization_params = [0, 1, 10, 100, 1000, 10000]\n",
    "ridge_results = []\n",
    "\n",
    "for lambda_val in regularization_params:\n",
    "    # Adjust learning rate based on lambda\n",
    "    if lambda_val <= 10:\n",
    "        lr = 0.1\n",
    "    elif lambda_val <= 100:\n",
    "        lr = 0.01\n",
    "    else:\n",
    "        lr = 0.001\n",
    "    \n",
    "    model_name = f\"Ridge Regression (λ={lambda_val})\" if lambda_val > 0 else \"Linear Regression (λ=0)\"\n",
    "    \n",
    "    # Train model (same function for all - when λ=0, it's standard regression)\n",
    "    theta_learned = train_ridge_with_gd(\n",
    "        X_train, y_train, \n",
    "        learning_rate=lr, \n",
    "        iterations=max_iterations,\n",
    "        lambda_param=lambda_val\n",
    "    )\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    test_predictions = X_test @ theta_learned\n",
    "    \n",
    "    # Check for numerical issues\n",
    "    if np.any(np.isnan(test_predictions)) or np.any(np.isinf(test_predictions)):\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Intercept: {theta_learned[0]:.4f}\")\n",
    "        print(f\"  Slope:     {theta_learned[1]:.4f}\")\n",
    "        print(f\"  MSE:       DIVERGED\")\n",
    "        print(f\"  R²:        DIVERGED\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_value = mean_squared_error(y_test, test_predictions)\n",
    "    r2_value = r2_score(y_test, test_predictions)\n",
    "    \n",
    "    # Store results\n",
    "    ridge_results.append({\n",
    "        'Lambda': lambda_val,\n",
    "        'Intercept': theta_learned[0],\n",
    "        'Slope': theta_learned[1],\n",
    "        'MSE': mse_value,\n",
    "        'R²': r2_value\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Intercept: {theta_learned[0]:>8.4f}\")\n",
    "    print(f\"  Slope:     {theta_learned[1]:>8.4f}\")\n",
    "    print(f\"  MSE:       {mse_value:>8.4f}\")\n",
    "    print(f\"  R²:        {r2_value:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99240769-106d-4a43-a469-581c7fcf33c1",
   "metadata": {},
   "source": [
    "With λ = 0 (standard linear regression), the estimated slope is 1.9378, close \n",
    "to the true value of 2.0, achieving MSE = 1.7147 and R² = 0.7517. For small \n",
    "regularization values (λ = 1 and λ = 10), the slope remains near 1.93-1.90 \n",
    "and performance metrics stay nearly constant, indicating that mild regularization \n",
    "has minimal effect on the model. As λ increases to 100, ridge regression begins \n",
    "to shrink the slope to 1.6387, causing MSE to increase to 1.8658 and R² to \n",
    "decrease to 0.7299, showing the bias-variance tradeoff where regularization \n",
    "prevents overfitting but reduces model fit. At λ = 1000, the slope is heavily \n",
    "penalized to 0.6859, MSE jumps to 3.9471, and R² drops to 0.4285, demonstrating \n",
    "significant underfitting as the model becomes too simple. When λ = 10000, the \n",
    "slope shrinks nearly to zero (0.1007), MSE increases to 6.4353, and R² drops \n",
    "to 0.0683, indicating poor performance with excessive regularization. In \n",
    "conclusion, as the regularization parameter λ increases, coefficients shrink \n",
    "toward zero, MSE increases, and R² decreases. Small λ values maintain good \n",
    "performance while providing regularization benefits, but excessive regularization \n",
    "causes severe underfitting where the model ignores the true relationship between \n",
    "variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
