{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13715b0-9cab-4eeb-b708-98b48f02558d",
   "metadata": {},
   "source": [
    "**<h3>Problem 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ea3eff-6086-449a-8def-795c98971efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d29823-a07a-4e4b-885d-ffbfb3a5d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COEFFICIENTS:\n",
      "Intercept: 520.4148\n",
      "bedrooms: -12.5220\n",
      "bathrooms: 18.5276\n",
      "sqft_living: 56.7488\n",
      "sqft_lot: 10.8819\n",
      "floors: 8.0437\n",
      "waterfront: 63.7429\n",
      "view: 48.2001\n",
      "condition: 12.9643\n",
      "grade: 92.2315\n",
      "sqft_above: 48.2901\n",
      "sqft_basement: 27.1370\n",
      "yr_built: -67.6431\n",
      "yr_renovated: 17.2714\n",
      "lat: 78.3757\n",
      "long: -1.0352\n",
      "sqft_living15: 45.5777\n",
      "sqft_lot15: -12.9301\n",
      "\n",
      "TRAINING METRICS:\n",
      "MSE: 31486.1678\n",
      "R²: 0.7265\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Drop columns\n",
    "columns_to_drop = ['id', 'date', 'zipcode', 'Unnamed: 0']\n",
    "train_df = train_df.drop(columns=[col for col in columns_to_drop if col in train_df.columns])\n",
    "test_df = test_df.drop(columns=[col for col in columns_to_drop if col in test_df.columns])\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop('price', axis=1)\n",
    "y_train = train_df['price'] / 1000  \n",
    "\n",
    "X_test = test_df.drop('price', axis=1)\n",
    "y_test = test_df['price'] / 1000\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on training set\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# Calculate training metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"COEFFICIENTS:\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "for feature, coef in zip(X_train.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\nTRAINING METRICS:\")\n",
    "print(f\"MSE: {train_mse:.4f}\")\n",
    "print(f\"R²: {train_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab771cd-9303-434f-aca8-6cd7f75dfd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING METRICS:\n",
      "MSE: 57628.1547\n",
      "R²: 0.6544\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTESTING METRICS:\")\n",
    "print(f\"MSE: {test_mse:.4f}\")\n",
    "print(f\"R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c2bcd-36db-4091-b5bd-12b34b7f9f1c",
   "metadata": {},
   "source": [
    "The features that contribute the most to the linear regression model is grade (92.23), lat (78.38), yr_built (-67.64), waterfront (63.74), sqft_living (56.75). These features contributing makes sense since house quality, location, having a waterfront view, and square footage are all obvious drivers of housing price. \n",
    "\n",
    "The training R² of 0.7265 indicates the model explains 72.65% of the variance in house prices, which is moderately good. The testing R² of 0.6544 represents that the model explains 65% of price variation. suggesting some overfitting. The model is decent but there's is definetely room to improve the model.\n",
    "\n",
    "There's a bit of overfitting (R² drops from 0.73 to 0.65). The model generalizes okay overall, though individual predictions can still be off by a fair amount. The model generalizes okay overall, though individual predictions can still be off by a fair amount.\n",
    "\n",
    "The testing MSE (57,628.15) is significantly higher than the training MSE (31,486.17), indicating the model overfits to the training data and doesn't generalize as well to new houses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebcc5f-fe46-49d6-bd6f-73f0bbc01bd2",
   "metadata": {},
   "source": [
    "**<h3>Problem 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe54ba28-1b7c-4afe-87eb-e48b25029419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COEFFICIENTS:\n",
      "Intercept: 520.4148\n",
      "bedrooms: -12.5220\n",
      "bathrooms: 18.5276\n",
      "sqft_living: 56.7488\n",
      "sqft_lot: 10.8819\n",
      "floors: 8.0437\n",
      "waterfront: 63.7429\n",
      "view: 48.2001\n",
      "condition: 12.9643\n",
      "grade: 92.2315\n",
      "sqft_above: 48.2901\n",
      "sqft_basement: 27.1370\n",
      "yr_built: -67.6431\n",
      "yr_renovated: 17.2714\n",
      "lat: 78.3757\n",
      "long: -1.0352\n",
      "sqft_living15: 45.5777\n",
      "sqft_lot15: -12.9301\n",
      "\n",
      "TRAINING METRICS:\n",
      "MSE: 31486.1678\n",
      "R²: 0.7265\n",
      "\n",
      "TESTING METRICS:\n",
      "MSE: 57628.1547\n",
      "R²: 0.6544\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "X_train_np = X_train_scaled\n",
    "y_train_np = y_train.to_numpy(dtype=float)\n",
    "X_test_np = X_test_scaled\n",
    "y_test_np = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Add bias term \n",
    "X_train_b = np.c_[np.ones((X_train_np.shape[0], 1)), X_train_np]\n",
    "X_test_b = np.c_[np.ones((X_test_np.shape[0], 1)), X_test_np]\n",
    "\n",
    "# Closed-form solution\n",
    "theta = np.linalg.pinv(X_train_b) @ y_train_np\n",
    "\n",
    "# Function to predict on a new testing point\n",
    "def predict_point(x_new, theta):\n",
    "    x_new = np.array(x_new, dtype=float).reshape(1, -1)\n",
    "    x_new_b = np.c_[np.ones((1, 1)), x_new]\n",
    "    return float(x_new_b @ theta)\n",
    "\n",
    "y_train_pred_cf = X_train_b @ theta\n",
    "y_test_pred_cf = X_test_b @ theta\n",
    "\n",
    "# Calculate metrics\n",
    "mse_train_cf = mean_squared_error(y_train_np, y_train_pred_cf)\n",
    "r2_train_cf = r2_score(y_train_np, y_train_pred_cf)\n",
    "mse_test_cf = mean_squared_error(y_test_np, y_test_pred_cf)\n",
    "r2_test_cf = r2_score(y_test_np, y_test_pred_cf)\n",
    "\n",
    "print(\"\\nCOEFFICIENTS:\")\n",
    "print(f\"Intercept: {theta[0]:.4f}\")\n",
    "for feature, coef in zip(X_train.columns, theta[1:]):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\nTRAINING METRICS:\")\n",
    "print(f\"MSE: {mse_train_cf:.4f}\")\n",
    "print(f\"R²: {r2_train_cf:.4f}\")\n",
    "\n",
    "print(f\"\\nTESTING METRICS:\")\n",
    "print(f\"MSE: {mse_test_cf:.4f}\")\n",
    "print(f\"R²: {r2_test_cf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e4afd7-ad2b-4207-840b-8739934430cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING SET:\n",
      "  Closed-Form MSE:    31486.1678\n",
      "  Sklearn MSE:        31486.1678\n",
      "  Closed-Form R²:         0.7265\n",
      "  Sklearn R²:             0.7265\n",
      "\n",
      "TESTING SET:\n",
      "  Closed-Form MSE:    57628.1547\n",
      "  Sklearn MSE:        57628.1547\n",
      "  Closed-Form R²:         0.6544\n",
      "  Sklearn R²:             0.6544\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAINING SET:\")\n",
    "print(f\"  Closed-Form MSE:  {mse_train_cf:>12.4f}\")\n",
    "print(f\"  Sklearn MSE:      {train_mse:>12.4f}\")\n",
    "print(f\"  Closed-Form R²:   {r2_train_cf:>12.4f}\")\n",
    "print(f\"  Sklearn R²:       {train_r2:>12.4f}\")\n",
    "\n",
    "print(\"\\nTESTING SET:\")\n",
    "print(f\"  Closed-Form MSE:  {mse_test_cf:>12.4f}\")\n",
    "print(f\"  Sklearn MSE:      {test_mse:>12.4f}\")\n",
    "print(f\"  Closed-Form R²:   {r2_test_cf:>12.4f}\")\n",
    "print(f\"  Sklearn R²:       {test_r2:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1ad37-8225-4f34-bb9a-13d09f7d003b",
   "metadata": {},
   "source": [
    "The closed-form implementation produces identical results to sklearn's Linear Regression on both training and testing sets. The training MSE(31,486.17) and R² (0.7265), as well as testing MSE (57,628.15) and R²(0.6544), match exactly across both methods. This confirms that my implementation correctly applies the least squares formula which is the same mathematical solution used by sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af304328-1dcb-4820-b2b0-b1013697b716",
   "metadata": {},
   "source": [
    "**<h3>Problem 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9ff1c6-bf03-45e4-975c-57b85498b14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression Results (Feature: sqft_living)\n",
      "\n",
      "Polynomial Degree p = 1:\n",
      "  Training MSE:    57947.5262\n",
      "  Training R²:         0.4967\n",
      "  Testing MSE:     88575.9785\n",
      "  Testing R²:          0.4687\n",
      "\n",
      "Polynomial Degree p = 2:\n",
      "  Training MSE:    54822.6651\n",
      "  Training R²:         0.5238\n",
      "  Testing MSE:     71791.6795\n",
      "  Testing R²:          0.5694\n",
      "\n",
      "Polynomial Degree p = 3:\n",
      "  Training MSE:    53785.1947\n",
      "  Training R²:         0.5329\n",
      "  Testing MSE:     99833.4838\n",
      "  Testing R²:          0.4012\n",
      "\n",
      "Polynomial Degree p = 4:\n",
      "  Training MSE:    52795.7748\n",
      "  Training R²:         0.5415\n",
      "  Testing MSE:    250979.2743\n",
      "  Testing R²:         -0.5053\n",
      "\n",
      "Polynomial Degree p = 5:\n",
      "  Training MSE:    52626.1120\n",
      "  Training R²:         0.5429\n",
      "  Testing MSE:    570616.9148\n",
      "  Testing R²:         -2.4225\n"
     ]
    }
   ],
   "source": [
    "feature_name = \"sqft_living\"\n",
    "feature_index = list(X_train.columns).index(feature_name)\n",
    "\n",
    "X_train_single = X_train_scaled[:, [feature_index]]\n",
    "X_test_single = X_test_scaled[:, [feature_index]]\n",
    "\n",
    "y_train_poly = y_train.to_numpy(dtype=float)\n",
    "y_test_poly = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Create polynomial features\n",
    "def build_polynomial_matrix(X, degree):\n",
    "    poly_terms = [X ** power for power in range(1, degree + 1)]\n",
    "    return np.concatenate(poly_terms, axis=1)\n",
    "\n",
    "# Fit polynomial regression using closed-form solution\n",
    "def fit_polynomial_model(X_data, y_data, degree):\n",
    "    X_poly = build_polynomial_matrix(X_data, degree)\n",
    "    X_with_bias = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]\n",
    "    coefficients = np.linalg.pinv(X_with_bias) @ y_data\n",
    "    return coefficients\n",
    "\n",
    "# Make predictions using learned coefficients\n",
    "def make_predictions(X_data, coefficients, degree):\n",
    "    X_poly = build_polynomial_matrix(X_data, degree)\n",
    "    X_with_bias = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]\n",
    "    predictions = X_with_bias @ coefficients    \n",
    "    return predictions\n",
    "\n",
    "# Train model and compute MSE and R² metrics for given polynomial degree\n",
    "def compute_metrics(X_train_data, y_train_data, X_test_data, y_test_data, degree):\n",
    "    coefficients = fit_polynomial_model(X_train_data, y_train_data, degree)\n",
    "    train_predictions = make_predictions(X_train_data, coefficients, degree)\n",
    "    test_predictions = make_predictions(X_test_data, coefficients, degree)\n",
    "    \n",
    "    train_mse_value = mean_squared_error(y_train_data, train_predictions)\n",
    "    train_r2_value = r2_score(y_train_data, train_predictions)\n",
    "    \n",
    "    test_mse_value = mean_squared_error(y_test_data, test_predictions)\n",
    "    test_r2_value = r2_score(y_test_data, test_predictions)\n",
    "    \n",
    "    return {\n",
    "        'p': degree,\n",
    "        'MSE_train': train_mse_value,\n",
    "        'R2_train': train_r2_value,\n",
    "        'MSE_test': test_mse_value,\n",
    "        'R2_test': test_r2_value\n",
    "    }\n",
    "\n",
    "polynomial_degrees = [1, 2, 3, 4, 5]\n",
    "model_results = []\n",
    "\n",
    "print(f\"Polynomial Regression Results (Feature: {feature_name})\")\n",
    "\n",
    "for deg in polynomial_degrees:\n",
    "    metrics = compute_metrics(X_train_single, y_train_poly, \n",
    "                              X_test_single, y_test_poly, deg)\n",
    "    model_results.append(metrics)\n",
    "    \n",
    "    print(f\"\\nPolynomial Degree p = {deg}:\")\n",
    "    print(f\"  Training MSE:  {metrics['MSE_train']:>12.4f}\")\n",
    "    print(f\"  Training R²:   {metrics['R2_train']:>12.4f}\")\n",
    "    print(f\"  Testing MSE:   {metrics['MSE_test']:>12.4f}\")\n",
    "    print(f\"  Testing R²:    {metrics['R2_test']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e80ed3-4424-4b9b-88e4-2d60a115fc18",
   "metadata": {},
   "source": [
    "As the polynomial degree increases from 1 to 5, the training MSE decreases from 57,947.53 to 52,626.11 and training R² increases from 0.497 to 0.543, indicating that higher-degree polynomials fit the training data more closely. On the testing set, however, the pattern is different. Performance improves from degree 1 to degree 2, the testing MSE decreases from 88,575.98 to 71,791.68, and R² increases from 0.469 to 0.569, showing that a quadratic relationship captures the data better than a linear one. For degrees 3 and higher, testing performance decreases sharply. At degree 4, testing MSE jumps to 250,979.27 with R² = -0.505, and at degree 5, testing MSE reaches 570,616.91 with R² = -2.42. Negative R² values indicate the model performs worse than simply predicting the mean price, demonstrating severe overfitting. The optimal polynomial degree appears to be p = 2, which achieves the best balance between model complexity and generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b395165-98a3-4186-83b8-14852c7f72d0",
   "metadata": {},
   "source": [
    "**<h3>Problem 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e287f74-5ec3-4c46-ac53-8833f304e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent to optimize theta\n",
    "def train_with_gradient_descent(X_matrix, y_vector, learning_rate, iterations):\n",
    "    num_samples, num_features = X_matrix.shape\n",
    "    \n",
    "    parameters = np.zeros(num_features)\n",
    "    \n",
    "    # Gradient descent iterations\n",
    "    for iteration in range(iterations):\n",
    "        predictions = X_matrix @ parameters\n",
    "        residuals = predictions - y_vector\n",
    "        grad = (2.0/num_samples) * (X_matrix.T @ residuals)\n",
    "        parameters = parameters - learning_rate * grad\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Calculate performance metrics for given parameters\n",
    "def calculate_performance_metrics(X_train_matrix, y_train_vector, X_test_matrix, y_test_vector, parameters):\n",
    "    train_preds = X_train_matrix @ parameters\n",
    "    test_preds = X_test_matrix @ parameters\n",
    "    \n",
    "    performance = {\n",
    "        \"train_mse\": mean_squared_error(y_train_vector, train_preds),\n",
    "        \"train_r2\": r2_score(y_train_vector, train_preds),\n",
    "        \"test_mse\": mean_squared_error(y_test_vector, test_preds),\n",
    "        \"test_r2\": r2_score(y_test_vector, test_preds)\n",
    "    }\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a055759-2b37-4baa-bfb9-7d00ad5e96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate α = 0.01, Iterations = 10\n",
      "  θ₀ (intercept): 95.1980\n",
      "  Theta: [11.9286 20.2833 32.1165  5.3808  9.5255]\n",
      "  Train MSE: 235727.7698, Train R²: -1.0474\n",
      "  Test MSE: 280568.7055, Test R²: -0.6828\n",
      "\n",
      "Learning Rate α = 0.01, Iterations = 50\n",
      "  θ₀ (intercept): 330.8955\n",
      "  Theta: [ 6.0053 23.8214 54.6659  3.6375 11.122 ]\n",
      "  Train MSE: 69720.4989, Train R²: 0.3945\n",
      "  Test MSE: 97049.5408, Test R²: 0.4179\n",
      "\n",
      "Learning Rate α = 0.01, Iterations = 100\n",
      "  θ₀ (intercept): 451.3976\n",
      "  Theta: [-3.6569 19.2418 56.955   2.6937 10.4482]\n",
      "  Train MSE: 36820.3499, Train R²: 0.6802\n",
      "  Test MSE: 63333.0351, Test R²: 0.6201\n",
      "\n",
      "Learning Rate α = 0.1, Iterations = 10\n",
      "  θ₀ (intercept): 464.5357\n",
      "  Theta: [-4.6456 18.7334 57.1255  2.53   10.5525]\n",
      "  Train MSE: 35105.1019, Train R²: 0.6951\n",
      "  Test MSE: 61630.4335, Test R²: 0.6304\n",
      "\n",
      "Learning Rate α = 0.1, Iterations = 50\n",
      "  θ₀ (intercept): 520.4074\n",
      "  Theta: [-12.4572  17.6212  57.1878   7.8533   8.006 ]\n",
      "  Train MSE: 31497.2612, Train R²: 0.7264\n",
      "  Test MSE: 57722.4753, Test R²: 0.6538\n",
      "\n",
      "Learning Rate α = 0.1, Iterations = 100\n",
      "  θ₀ (intercept): 520.4148\n",
      "  Theta: [-12.5191  18.4242  56.7696  10.2707   8.061 ]\n",
      "  Train MSE: 31486.4318, Train R²: 0.7265\n",
      "  Test MSE: 57638.9572, Test R²: 0.6543\n",
      "\n",
      "Learning Rate α = 0.5, Iterations = 10\n",
      "  θ₀ (intercept): 520.4148\n",
      "  Theta: [-40545159.6361 -60243904.6471 -66812541.2293 -25606793.6371\n",
      " -37406765.8138]\n",
      "  Train MSE: 145606446341105152.0000, Train R²: -1264634536089.7981\n",
      "  Test MSE: 162606792167659168.0000, Test R²: -975288001371.4670\n",
      "\n",
      "Learning Rate α = 0.5, Iterations = 50\n",
      "  θ₀ (intercept): 2270423612371449344.0000\n",
      "  Theta: [-3.7710e+32 -5.6031e+32 -6.2140e+32 -2.3816e+32 -3.4791e+32]\n",
      "  Train MSE: 12595424869718312266148462372181871955673249275964149318615119691776.0000, Train R²: -109394945672031152795833159794633738969624184012889409743486976.0000\n",
      "  Test MSE: 14066007335069655033392642976568518810352536260830497729528949899264.0000, Test R²: -84365529866463939620679378782343619347503272391636626278711296.0000\n",
      "\n",
      "Learning Rate α = 0.5, Iterations = 100\n",
      "  θ₀ (intercept): 37209535223949695174238061762923529173910859808768.0000\n",
      "  Theta: [-6.1249e+63 -9.1007e+63 -1.0093e+64 -3.8683e+63 -5.6508e+63]\n",
      "  Train MSE: 3322791536432718343625110344941455770939886881950077823585257216793495294189348828657316715762052214054477262152867308221006610432.0000, Train R²: -28859415491529308314838563486984411054836367985747297775056876352117299314048670328818065603621869977438921728672338844581888.0000\n",
      "  Test MSE: 3710745021133635037204839720337598494233349666747046381130279649842748866360566736699000194583798041535978075069630617616274948096.0000, Test R²: -22256420208651323060078186123877166732469720665639249698866579737768571510323671244843347298706006756143686681149689787580416.0000\n",
      "\n",
      "SUMMARY TABLE\n",
      " Learning Rate  Iterations     Train MSE       Train R²      Test MSE        Test R²\n",
      "          0.01          10  2.357278e+05  -1.047365e+00  2.805687e+05  -6.828036e-01\n",
      "          0.01          50  6.972050e+04   3.944571e-01  9.704954e+04   4.179133e-01\n",
      "          0.01         100  3.682035e+04   6.802045e-01  6.333304e+04   6.201392e-01\n",
      "          0.10          10  3.510510e+04   6.951019e-01  6.163043e+04   6.303511e-01\n",
      "          0.10          50  3.149726e+04   7.264371e-01  5.772248e+04   6.537904e-01\n",
      "          0.10         100  3.148643e+04   7.265311e-01  5.763896e+04   6.542913e-01\n",
      "          0.50          10  1.456064e+17  -1.264635e+12  1.626068e+17  -9.752880e+11\n",
      "          0.50          50  1.259542e+67  -1.093949e+62  1.406601e+67  -8.436553e+61\n",
      "          0.50         100 3.322792e+129 -2.885942e+124 3.710745e+129 -2.225642e+124\n"
     ]
    }
   ],
   "source": [
    "X_train_with_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_with_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "y_train_array = y_train.to_numpy(dtype=float)\n",
    "y_test_array = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Define experiment parameters\n",
    "learning_rate_values = [0.01, 0.1, 0.5]\n",
    "iteration_counts = [10, 50, 100]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for lr in learning_rate_values:\n",
    "    for iter_count in iteration_counts:\n",
    "        # Train model\n",
    "        learned_params = train_with_gradient_descent(\n",
    "            X_train_with_bias,\n",
    "            y_train_array,\n",
    "            learning_rate=lr,\n",
    "            iterations=iter_count\n",
    "        )\n",
    "        \n",
    "        # Evaluate performance\n",
    "        metrics = calculate_performance_metrics(\n",
    "            X_train_with_bias, y_train_array,\n",
    "            X_test_with_bias, y_test_array,\n",
    "            learned_params\n",
    "        )\n",
    "        \n",
    "        # Print theta values\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "        print(f\"\\nLearning Rate α = {lr}, Iterations = {iter_count}\")\n",
    "        print(f\"  θ₀ (intercept): {learned_params[0]:.4f}\")\n",
    "        print(f\"  Theta: {learned_params[1:6]}\")\n",
    "        print(f\"  Train MSE: {metrics['train_mse']:.4f}, Train R²: {metrics['train_r2']:.4f}\")\n",
    "        print(f\"  Test MSE: {metrics['test_mse']:.4f}, Test R²: {metrics['test_r2']:.4f}\")\n",
    "        \n",
    "        # Store for table\n",
    "        experiment_results.append({\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Iterations\": iter_count,\n",
    "            \"Train MSE\": metrics[\"train_mse\"],\n",
    "            \"Train R²\": metrics[\"train_r2\"],\n",
    "            \"Test MSE\": metrics[\"test_mse\"],\n",
    "            \"Test R²\": metrics[\"test_r2\"]\n",
    "        })\n",
    "\n",
    "print(\"\\nSUMMARY TABLE\")\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c10ad-0686-4e9b-ac39-800cdb40a26a",
   "metadata": {},
   "source": [
    "For α = 0.01, the algorithm improves as iterations increase. The training MSE decreases from 235,728 to 36,820, and R² increases from -1.05 to 0.68 by 100 iterations, showing steady but slow convergence. For α = 0.1, the algorithm converges much faster and achieves optimal performance. By 50 iterations, training R² reaches 0.7264, and by 100 iterations it achieves 0.7265, exactly matching the optimal solution from Problem 3. The testing R² also stabilizes at 0.6543, demonstrating effective convergence within 50-100 iterations. For α = 0.5, the algorithm diverges catastrophically. MSE explodes to 10¹²⁹ and R² becomes -10¹²⁴, showing the learning rate is far too large and causes extreme overshooting. In conclusion, gradient descent converges when the learning rate is appropriately chosen (α = 0.1), requires around 50-100 iterations to reach the optimal solution, and fails completely when the learning rate is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a26754-ade1-4675-8fed-04efaf0e86f9",
   "metadata": {},
   "source": [
    "**<h3>Problem 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "531ba9e0-56e2-4c8d-92f7-b7923ec37e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_with_gd(X_matrix, y_vector, learning_rate, iterations, lambda_param):\n",
    "    num_samples, num_features = X_matrix.shape\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = np.zeros(num_features)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Predictions\n",
    "        predictions = X_matrix @ parameters\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = predictions - y_vector\n",
    "        \n",
    "        # Standard gradient\n",
    "        grad = (2.0 / num_samples) * (X_matrix.T @ residuals)\n",
    "        \n",
    "        # Add ridge penalty (L2 regularization) - don't penalize intercept\n",
    "        parameters_for_penalty = parameters.copy()\n",
    "        parameters_for_penalty[0] = 0  # Exclude intercept from penalty\n",
    "        ridge_penalty = (2 * lambda_param / num_samples) * parameters_for_penalty\n",
    "        grad = grad + ridge_penalty\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = parameters - learning_rate * grad\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f4bd89-0826-4baa-9978-91ef85e0cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression (λ=0):\n",
      "  Intercept:   1.1377\n",
      "  Slope:       1.9453\n",
      "  MSE:         1.9499\n",
      "  R²:          0.7258\n",
      "\n",
      "Ridge Regression (λ=1):\n",
      "  Intercept:   1.1377\n",
      "  Slope:       1.9439\n",
      "  MSE:         1.9499\n",
      "  R²:          0.7258\n",
      "\n",
      "Ridge Regression (λ=10):\n",
      "  Intercept:   1.1372\n",
      "  Slope:       1.9311\n",
      "  MSE:         1.9502\n",
      "  R²:          0.7258\n",
      "\n",
      "Ridge Regression (λ=100):\n",
      "  Intercept:   1.1325\n",
      "  Slope:       1.8124\n",
      "  MSE:         1.9740\n",
      "  R²:          0.7224\n",
      "\n",
      "Ridge Regression (λ=1000):\n",
      "  Intercept:   1.1056\n",
      "  Slope:       1.1224\n",
      "  MSE:         2.8735\n",
      "  R²:          0.5960\n",
      "\n",
      "Ridge Regression (λ=10000):\n",
      "  Intercept:   1.0710\n",
      "  Slope:       0.2335\n",
      "  MSE:         5.9471\n",
      "  R²:          0.1638\n"
     ]
    }
   ],
   "source": [
    "def train_ridge_with_gd(X_matrix, y_vector, learning_rate, iterations, lambda_param):\n",
    "    num_samples, num_features = X_matrix.shape\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = np.zeros(num_features)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Predictions\n",
    "        predictions = X_matrix @ parameters\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = predictions - y_vector\n",
    "        \n",
    "        # Standard gradient\n",
    "        grad = (2.0 / num_samples) * (X_matrix.T @ residuals)\n",
    "        \n",
    "        # Add ridge penalty \n",
    "        parameters_for_penalty = parameters.copy()\n",
    "        parameters_for_penalty[0] = 0  # Exclude intercept from penalty\n",
    "        ridge_penalty = (2 * lambda_param / num_samples) * parameters_for_penalty\n",
    "        grad = grad + ridge_penalty\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = parameters - learning_rate * grad\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_size = 1000\n",
    "X_values = np.random.uniform(low=-2, high=2, size=sample_size)\n",
    "noise_values = np.random.normal(loc=0, scale=np.sqrt(2), size=sample_size)\n",
    "y_values = 1 + 2 * X_values + noise_values\n",
    "\n",
    "# Create design matrix with intercept\n",
    "design_matrix = np.column_stack([np.ones(sample_size), X_values])\n",
    "\n",
    "# Training parameters\n",
    "max_iterations = 5000\n",
    "regularization_params = [0, 1, 10, 100, 1000, 10000]\n",
    "ridge_results = []\n",
    "\n",
    "for lambda_val in regularization_params:\n",
    "    # Adjust learning rate based on lambda\n",
    "    if lambda_val <= 10:\n",
    "        lr = 0.1\n",
    "    elif lambda_val <= 100:\n",
    "        lr = 0.01\n",
    "    else:\n",
    "        lr = 0.001\n",
    "    \n",
    "    model_name = f\"Ridge Regression (λ={lambda_val})\" if lambda_val > 0 else \"Linear Regression (λ=0)\"\n",
    "    \n",
    "    # Train model on all data\n",
    "    theta_learned = train_ridge_with_gd(\n",
    "        design_matrix, y_values, \n",
    "        learning_rate=lr, \n",
    "        iterations=max_iterations,\n",
    "        lambda_param=lambda_val\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = design_matrix @ theta_learned\n",
    "    \n",
    "    # Check for numerical issues\n",
    "    if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Intercept: {theta_learned[0]:.4f}\")\n",
    "        print(f\"  Slope:     {theta_learned[1]:.4f}\")\n",
    "        print(f\"  MSE:       DIVERGED\")\n",
    "        print(f\"  R²:        DIVERGED\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_value = mean_squared_error(y_values, predictions)\n",
    "    r2_value = r2_score(y_values, predictions)\n",
    "    \n",
    "    # Store results\n",
    "    ridge_results.append({\n",
    "        'Lambda': lambda_val,\n",
    "        'Intercept': theta_learned[0],\n",
    "        'Slope': theta_learned[1],\n",
    "        'MSE': mse_value,\n",
    "        'R²': r2_value\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Intercept: {theta_learned[0]:>8.4f}\")\n",
    "    print(f\"  Slope:     {theta_learned[1]:>8.4f}\")\n",
    "    print(f\"  MSE:       {mse_value:>8.4f}\")\n",
    "    print(f\"  R²:        {r2_value:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99240769-106d-4a43-a469-581c7fcf33c1",
   "metadata": {},
   "source": [
    "With λ = 0 (standard linear regression), the estimated slope is 1.9378, close to the true value of 2.0, achieving MSE = 1.7147 and R² = 0.7517. For small regularization values (λ = 1 and λ = 10), the slope remains near 1.94-1.92 (1.9360 and 1.9203) and performance metrics stay nearly constant, indicating that mild regularization has minimal effect on the model. As λ increases to 100, ridge regression begins to shrink the slope to 1.7758, causing MSE to increase to 1.7667 and R² to decrease to 0.7442, showing the bias-variance tradeoff \n",
    "where regularization prevents overfitting but slightly reduces model fit. At λ = 1000, the slope is more heavily penalized to 1.0132, MSE increases to 2.9586, and R² drops to 0.5716, demonstrating increased underfitting as the model becomes simpler. When λ = 10000, the slope shrinks significantly to 0.1914, MSE increases to 5.9924, and R² drops to 0.1324, indicating substantial underfitting with excessive regularization. In conclusion, as the regularization parameter λ increases, coefficients shrink toward zero, MSE increases, and R² decreases. Small λ values maintain good performance while providing regularization benefits, but excessive regularization causes severe underfitting where the model increasingly ignores the true relationship between variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
